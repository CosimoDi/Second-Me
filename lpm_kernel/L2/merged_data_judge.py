import json
import concurrent.futures
from typing import Dict, Any
from dataclasses import dataclass
from tqdm import tqdm

try:
    from ollama import Client as OllamaClient
except ImportError:
    OllamaClient = None

@dataclass
class JudgeResult:
    score: float
    reasoning: str
    quality_level: str
    suggestions: str

class MergedDataJudge:
    """LLM Judge for evaluating merged.json data using Ollama Gemma"""

    def __init__(self, model_name: str = "gemma:2b", ollama_host: str = "http://localhost:11434", user_bio: str = None):
        """
        Initialize Judge with Ollama Gemma
        
        Args:
            model_name: Ollama model name (default: "gemma:2b")
            ollama_host: Ollama server host (default: http://localhost:11434)
            user_bio: User background information
        """
        self.model_name = model_name
        
        # Initialize Ollama client
        if OllamaClient is None:
            raise ImportError("ollama package is not installed.")
        
        # Ensure Ollama model is available
        print(f"Ensuring Ollama model {model_name} is available...")
        try:
            from lpm_kernel.L2.utils import save_ollama_model
            save_ollama_model(model_name)
            print(f"‚úÖ Ollama model {model_name} is ready")
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: Could not ensure Ollama model availability: {str(e)}")
            print("Continuing anyway - model might already be available")
        
        self.ollama_client = OllamaClient(host=ollama_host)
        
        # Judge evaluation prompt
        self.judge_prompt = """You are a professional AI model evaluation expert. Your task is to assess the quality of training data generated by the Second-Me project.

        Evaluation Criteria:
        1. **Relevance** (0-100 points): Whether the data is highly relevant to the user's background and needs
        2. **Accuracy** (0-100 points): Whether the information is accurate and truthful
        3. **Completeness** (0-100 points): Whether the response is complete and comprehensive
        4. **Personalization** (0-100 points): Whether it reflects personalized characteristics
        5. **Practicality** (0-100 points): Whether it provides practical help to the user

        Please evaluate the following data:

        User Background Information:
        {user_bio}

        Training Data:
        User Question: {user_question}
        AI Response: {ai_response}

        Please provide detailed evaluation results including:
        1. Individual scores (0-100 points)
        2. Overall score (0-100 points)
        3. Evaluation reasoning
        4. Quality level (excellent/good/fair/poor)
        5. Improvement suggestions

        Please return in JSON format:
        {{
        "score": overall_score,
        "reasoning": "detailed_evaluation_reasoning",
        "quality_level": "quality_level",
        "suggestions": "improvement_suggestions"
        }}"""

    def evaluate_single_item(self, item: Dict[str, Any], user_bio: str) -> JudgeResult:
        """
        Evaluate a single data item using Ollama Gemma
        
        Args:
            item: Single data item from merged.json
            user_bio: User background information
            
        Returns:
            JudgeResult: Evaluation result
        """
        try:
            # Extract user question and AI response
            user_question = item.get('user', '')
            ai_response = item.get('assistant', '')
            
            # Construct prompt
            prompt = self.judge_prompt.format(
                user_bio=user_bio,
                user_question=user_question,
                ai_response=ai_response
            )
            
            # Call Ollama API
            if self.ollama_client is None:
                raise RuntimeError("Ollama client not initialized")
            response = self.ollama_client.chat(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "You are a professional AI model evaluation expert."},
                    {"role": "user", "content": prompt}
                ],
                options={
                    "temperature": 0.1,
                    "num_predict": 1000
                }
            )
            result_text = response['message']['content']
            
            # Parse result
            if result_text:
                try:
                    start = result_text.find("{")
                    end = result_text.rfind("}") + 1
                    if start != -1 and end != -1:
                        json_text = result_text[start:end]
                        result_dict = json.loads(json_text)
                        print("-" * 60)
                        print("Correct JSON format:")
                        print(result_text)
                        print("-" * 60)
                        return JudgeResult(**result_dict)
                    else:
                        print("-" * 60)
                        print("No JSON found:")
                        print(result_text)
                        print("-" * 60)
                        raise json.JSONDecodeError("No JSON found", result_text or "", 0)
                    
                except json.JSONDecodeError:
                    # If JSON parsing fails, return default result
                    print("-" * 20)
                    print("ERROR: JSON parsing failed, unable to evaluate")
                    print(result_text)
                    print("-" * 20)
                    return JudgeResult(
                        score=0.0,
                        reasoning="JSON parsing failed, unable to evaluate",
                        quality_level="fair",
                        suggestions="Need to manually check data format"
                    )
            else:
                return JudgeResult(
                    score=0.0,
                    reasoning="Empty response from model",
                    quality_level="poor",
                    suggestions="Check model configuration"
                )
                
        except Exception as e:
            return JudgeResult(
                score=0.0,
                reasoning=f"Error occurred during evaluation: {str(e)}",
                quality_level="poor",
                suggestions="Check API configuration and network connection"
            )

    def filter_and_score_data_concurrent(self, merged_json_path: str, output_path: str, user_bio: str, 
                                       keep_ratio: float = 0.8, max_workers: int = 5) -> None:
        """
        Filter merged.json data using concurrent processing for better performance
        
        Args:
            merged_json_path: Path to merged.json file
            user_bio: User background information
            keep_ratio: Ratio of data to keep (0.0-1.0), default 0.8 (80%)
            max_workers: Number of concurrent worker threads
        """
        try:
            # Load data
            with open(merged_json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except Exception as e:
            print(f"‚ùå Error loading {merged_json_path}: {str(e)}")
            print("Keeping original file unchanged and skipping data filtering")
            return
        
        total_items = len(data)
        keep_count = int(total_items * keep_ratio)
        
        print(f"Starting concurrent evaluation of {total_items} data items...")
        print(f"Will keep top {keep_ratio*100:.0f}% ({keep_count}) items")
        print(f"Using {max_workers} concurrent workers")
        print(f"Using Ollama model: {self.model_name}")
        
        # Concurrent evaluation
        try:
            scored_data = []
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                # Submit all evaluation tasks
                future_to_index = {
                    executor.submit(self.evaluate_single_item, item, user_bio): i 
                    for i, item in enumerate(data)
                }
                
                # Process results as they complete
                completed_count = 0
                for future in tqdm(concurrent.futures.as_completed(future_to_index), 
                                  total=len(data), desc="Evaluation Progress"):
                    index = future_to_index[future]
                    item = data[index]
                    
                    try:
                        result = future.result()
                        
                        # Add scoring fields to the original item
                        item['score'] = result.score
                        item['reasoning'] = result.reasoning
                        # item['quality_level'] = result.quality_level
                        # item['suggestions'] = result.suggestions
                        
                        scored_data.append(item)
                        
                    except Exception as e:
                        print(f"‚ùå Error evaluating item {index}: {str(e)}")
                        print("Keeping original file unchanged and skipping data filtering")
                        return
                    
                    completed_count += 1
        except Exception as e:
            print(f"‚ùå Error during concurrent evaluation: {str(e)}")
            print("Keeping original file unchanged and skipping data filtering")
            return
        
        # Sort by score (highest first)
        scored_data.sort(key=lambda x: x['score'], reverse=True)
        
        # Keep only the top items
        filtered_data = scored_data[:keep_count]
        
        # Calculate statistics
        if scored_data:
            avg_score = sum(item['score'] for item in scored_data) / len(scored_data)
            kept_avg_score = sum(item['score'] for item in filtered_data) / len(filtered_data)
            min_score = scored_data[-1]['score']  # Lowest score
            max_score = scored_data[0]['score']   # Highest score
            
            print(f"\nData filtering completed:")
            print(f"  Original items: {total_items}")
            print(f"  Kept items: {len(filtered_data)} ({keep_ratio*100:.0f}%)")
            print(f"  Removed items: {total_items - len(filtered_data)}")
            print(f"  Score range: {min_score:.2f} - {max_score:.2f}")
            print(f"  Original average score: {avg_score:.2f}")
            print(f"  Kept average score: {kept_avg_score:.2f}")
            
            # Log quality distribution (commented out since quality_level is not added to output)
            # quality_counts = {}
            # for item in filtered_data:
            #     quality = item['quality_level']
            #     quality_counts[quality] = quality_counts.get(quality, 0) + 1
            # print(f"  Quality distribution of kept items: {quality_counts}")
        
        # Save the filtered data back to the original file
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(filtered_data, f, ensure_ascii=False, indent=2)
            
            print(f"Filtered data saved to {output_path}")
        except Exception as e:
            print(f"‚ùå Error saving filtered data: {str(e)}")
            print("Keeping original file unchanged and skipping data filtering")
            return

    def cleanup(self):
        """Release the Ollama model from memory"""
        try:
            from lpm_kernel.L2.utils import release_ollama_models_early
            release_ollama_models_early()
            print(f"‚úÖ Released Ollama models from memory")
        except Exception as e:
            print(f"‚ö†Ô∏è Could not release models: {str(e)}")

def main():
    """
    Test function for MergedDataJudge
    Uses existing merged.json as input and creates merged_test.json as output
    """
    import os
    import sys
    
    # Add the project root to Python path
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    sys.path.insert(0, project_root)
    
    # Import user bio function
    try:
        from lpm_kernel.base.database_operate import get_latest_global_bio
    except ImportError:
        print("Warning: Could not import get_latest_global_bio, using empty bio")
        get_latest_global_bio = lambda: None
    
    # Configuration
    input_file = "resources/data/merged.json"
    output_file = "resources/data/merged_test.json"
    keep_ratio = 0.8  # Keep 80% of the best data
    max_workers = 3   # Use 3 concurrent workers for testing
    
    print("=" * 60)
    print("MergedDataJudge Test")
    print("=" * 60)
    
    # Check if input file exists
    if not os.path.exists(input_file):
        print(f"‚ùå Error: Input file {input_file} does not exist!")
        print("Please run the training process first to generate merged.json")
        return False
    
    # Get user bio
    user_bio = ""
    try:
        bio_obj = get_latest_global_bio()
        if bio_obj and bio_obj.content_third_view:
            user_bio = bio_obj.content_third_view
            print(f"‚úÖ Found user bio ({len(user_bio)} characters)")
        else:
            print("‚ö†Ô∏è No user bio found, using empty bio")
    except Exception as e:
        print(f"‚ö†Ô∏è Warning: Could not get user bio: {str(e)}")
        print("Using empty bio")
    
    # Load and check input data
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"‚úÖ Loaded {len(data)} training examples from {input_file}")
    except Exception as e:
        print(f"‚ùå Error loading {input_file}: {str(e)}")
        return False
    
    # Initialize judge
    try:
        print(f"\nüîß Initializing MergedDataJudge...")
        judge = MergedDataJudge(
            model_name="gemma:2b",
            ollama_host="http://localhost:11434",
            user_bio=user_bio
        )
        print("‚úÖ MergedDataJudge initialized successfully")
    except Exception as e:
        print(f"‚ùå Error initializing MergedDataJudge: {str(e)}")
        return False
    
    # Run filtering
    try:
        print(f"\nüöÄ Starting data filtering...")
        print(f"Input: {input_file}")
        print(f"Output: {output_file}")
        print(f"Keep ratio: {keep_ratio*100:.0f}%")
        print(f"Max workers: {max_workers}")
        
        judge.filter_and_score_data_concurrent(
            merged_json_path=input_file,
            output_path=output_file,
            user_bio=user_bio,
            keep_ratio=keep_ratio,
            max_workers=max_workers
        )
        
        print(f"\n‚úÖ Test completed successfully!")
        print(f"Filtered data saved to: {output_file}")
        
        # Show file sizes
        input_size = os.path.getsize(input_file) / 1024  # KB
        output_size = os.path.getsize(output_file) / 1024  # KB
        print(f"Input file size: {input_size:.1f} KB")
        print(f"Output file size: {output_size:.1f} KB")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error during filtering: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    import sys
    success = main()
    sys.exit(0 if success else 1)
