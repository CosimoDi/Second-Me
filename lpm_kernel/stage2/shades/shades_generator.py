import re, os
from typing import List, Tuple, Dict, Any
import copy
import math
import time
import tiktoken
import traceback
import itertools
import statistics
import json
import random
from datetime import datetime
from collections import Counter
from typing import List, Dict, Any, Optional, Union, Tuple
from tqdm import tqdm  # æ·»åŠ tqdmå¯¼å…¥
from dotenv import load_dotenv
# from langfuse.decorators import observe

import openai
from requests.exceptions import Timeout
from concurrent.futures import ThreadPoolExecutor
from collections import defaultdict, OrderedDict
from lpm_kernel.api.services.user_llm_config_service import UserLLMConfigService
from lpm_kernel.file_data import Document
from lpm_kernel.file_data.document_repository import DocumentRepository

from lpm_kernel.configs.logging import get_train_process_logger

from .prompts import Prompts

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
logger = get_train_process_logger()

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(override=True)


class ShadeGenerate():
    _input_keys: List[str] = ["topics", "shades", "preferredLanguage"]
    _output_keys: List[str] = ["shades"]
    _must_keys: List[str] = ["topics", "shades", "preferredLanguage"]

    Shade_Generate_DEFAULT_PROMPT = {
        "shades_generate": Prompts.Shades_Generate_SYSTEM_PROMPT,
        "shades_update": Prompts.Shades_Update_SYSTEM_PROMPT,
        "shades_generate_zh": Prompts.Shades_Generate_zh_SYSTEM_PROMPT,
        "shades_update_zh": Prompts.Shades_Update_zh_SYSTEM_PROMPT
    }

    model_params = {
        "max_tokens": 30000,
        "top_p": 0.9,
        "temperature": 0.9,
        "request_timeout": 100,
        "response_format": {"type": "json_object"},
        "max_retries": 1
    }

    @property
    def _api_type(self):
        return "shades_generate"

    @property
    def input_keys(self) -> List[str]:
        return self._input_keys

    @property
    def output_keys(self) -> List[str]:
        return self._output_keys

    @property
    def must_keys(self) -> List[str]:
        return self._must_keys

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.prompts = Prompts()
        self.max_threads = 8
        self.user_llm_config_service = UserLLMConfigService()
        self.user_llm_config = self.user_llm_config_service.get_available_llm()

        self.llms: Dict[str, openai.OpenAI] = {
            k: openai.OpenAI(
                api_key=self.user_llm_config.chat_api_key,
                base_url=self.user_llm_config.chat_endpoint,
            )
            for k, _ in self.Shade_Generate_DEFAULT_PROMPT.items()
        }
        self.model_name = self.user_llm_config.chat_model_name

        self.class_name = self.__class__.__name__
        self._tokenizer = tiktoken.get_encoding("cl100k_base")
        self.model_params.update(**kwargs)

        self.langfuse_dict = {
            k: self._get_langfuse_prompt(k, v) for k, v in self.Shade_Generate_DEFAULT_PROMPT.items()
        }

    def _get_langfuse_prompt(self, prompt_key, default_prompt) -> Dict[str, Any]:
        # ç›´æ¥ä½¿ç”¨é»˜è®¤æç¤ºè¯ï¼Œä¸å†å°è¯•ä» langfuse è·å–
        system_prompt = default_prompt
        logger.info(f"Using local default prompt for {prompt_key}")
        return {
            "lf_prompt": None,
            "system_prompt": system_prompt
        }

    def _preprocess_topics_format(self, topics: List[dict], shades: List[dict]) -> tuple[str | Any, str | Any]:
        processed_topics = ""
        for topic in topics:
            cur_topic = f"topicName: {topic['topicName']}, topicDescription: {topic['topicDescription']}\n"
            processed_topics += cur_topic
        processed_shades = ""
        if shades == []:
            processed_shades = "No shades"
        else:
            for shade in shades:
                cur_shade = f"shadeName: {shade['shadeName']}, shadeIcon: {shade['shadeIcon']}, shadeDescription: {shade['shadeDescription']}\n"
                cur_shade += f"sourceTopics: {shade['sourceTopics']}\n"
                cur_shade += f"confidenceLevel: {shade['confidenceLevel']}\n"
                processed_shades += cur_shade
        return processed_topics, processed_shades

    def _clean_json_response(self, response: str) -> str:
        """
        æ¸…ç†LLMè¿”å›ç»“æœä¸­çš„markdownä»£ç å—æ ‡è®°

        Args:
            response: LLMè¿”å›çš„åŸå§‹å­—ç¬¦ä¸²

        Returns:
            str: æ¸…ç†åçš„JSONå­—ç¬¦ä¸²
        """
        # ç§»é™¤å¼€å¤´çš„```jsonæˆ–```
        if response.strip().startswith('```json'):
            response = response.strip()[7:]  # ç§»é™¤```json
        elif response.strip().startswith('```'):
            response = response.strip()[3:]  # ç§»é™¤```

        # ç§»é™¤ç»“å°¾çš„```
        if response.strip().endswith('```'):
            response = response.strip()[:-3]  # ç§»é™¤ç»“å°¾çš„```

        # Fix unquoted shadeIcon values
        import re
        # Pattern to match "shadeIcon": followed by an unquoted emoji or text
        # This handles cases like "shadeIcon": ğŸ™ï¸, or "shadeIcon": â¤ï¸
        pattern = r'"shadeIcon":\s*([^",\s][^,}\]]*?)(?=,|\s*[}\]])'
        
        def replace_icon(match):
            icon_value = match.group(1).strip()
            # If the icon is not already quoted, quote it
            if not (icon_value.startswith('"') and icon_value.endswith('"')):
                return f'"shadeIcon": "{icon_value}"'
            return match.group(0)
        
        response = re.sub(pattern, replace_icon, response)

        # Additional JSON cleaning - fix common issues
        # Fix trailing commas before closing brackets/braces
        response = re.sub(r',(\s*[}\]])', r'\1', response)
        
        # Note: Removed the general unquoted string fix as it was causing issues with array elements
        # The shadeIcon-specific fix should be sufficient for this use case

        return response.strip()

    # @observe(name="shades_generate")
    def shades_generate(self, topics, shades, preferredLanguage: str = "ç®€ä½“ä¸­æ–‡/Simplified Chinese") -> Dict[str, Any]:

        if isinstance(topics, dict):
            topics = topics.get("topics", [])
        if isinstance(shades, dict):
            shades = shades.get("shades", [])

        input_topics, input_shades = self._preprocess_topics_format(topics, shades)  # é¢„å¤„ç†topics å¤„ç†ä¸ºNLPå­—ç¬¦ä¸² æ–¹ä¾¿æ¨¡å‹ç†è§£
        logger.info(f"input topics: {input_topics}")
        logger.info(f"input shades: {input_shades}")

        # æ·»åŠ è¿›åº¦æ¡æ¥è·Ÿè¸ªshadeç”Ÿæˆè¿›åº¦
        total_steps = 4  # é¢„å¤„ç†ã€LLMè°ƒç”¨ã€åå¤„ç†ã€å»é‡
        with tqdm(total=total_steps, desc="ç”Ÿæˆshade", unit="æ­¥") as pbar:

            # æ­¥éª¤1: é¢„å¤„ç†å®Œæˆ
            pbar.set_description("é¢„å¤„ç†topicså’Œshades")
            pbar.update(1)

            # åˆ¤æ–­å½“å‰æ˜¯cold start è¿˜æ˜¯update
            if shades == []:
                logger.info("Shades Cold Start!")
                pbar.set_description("å†·å¯åŠ¨ç”Ÿæˆshade")

                if preferredLanguage == "ç®€ä½“ä¸­æ–‡/Simplified Chinese" or "chinese":
                    # è·å–prompt
                    message = Prompts.return_shades_generate_prompt(
                        system_prompt=self.langfuse_dict["shades_generate_zh"]["system_prompt"],
                        topics_list=input_topics,
                        prefer_lang=preferredLanguage
                    )
                    # å¯æœåŠ¡
                    llm = self.llms["shades_generate_zh"]
                else:
                    message = Prompts.return_shades_generate_prompt(
                        system_prompt=self.langfuse_dict["shades_generate"]["system_prompt"],
                        topics_list=input_topics,
                        prefer_lang=preferredLanguage
                    )
                    # å¯æœåŠ¡
                    llm = self.llms["shades_generate"]

                    # æ­¥éª¤2: è°ƒç”¨LLMç”Ÿæˆå†…å®¹
                pbar.set_description("è°ƒç”¨LLMç”Ÿæˆshade")
                try:
                    answer = llm.chat.completions.create(
                        model=self.model_name,
                        messages=message,
                    )
                    result = answer.choices[0].message.content
                    if result is None:
                        logger.error(f"LLM return Nothing!")
                        return {"shades": []}

                    # process llm result
                    try:
                        logger.info(f"llm return: result: {result}")
                        cleaned_result = self._clean_json_response(result)
                        pre_shades = json.loads(cleaned_result)

                    except json.JSONDecodeError as e:
                        logger.error(f"Failed to parse LLM response: {e}")
                        raise e
                except Exception as e:
                    logger.error(f"Cannot fetch LLM correctly: {e}")
                    raise e
                pbar.update(1)

            else:
                logger.info("Shades Update!")
                pbar.set_description("æ›´æ–°ç°æœ‰shade")

                if preferredLanguage == "ç®€ä½“ä¸­æ–‡/Simplified Chinese" or "chinese":
                    # è·å–prompt
                    message = Prompts.return_shades_update_prompt(
                        system_prompt=self.langfuse_dict["shades_update_zh"]["system_prompt"],
                        cur_shades=input_shades,
                        topics_list=input_topics,
                        prefer_lang=preferredLanguage
                    )
                    # å¯æœåŠ¡
                    llm = self.llms["shades_update_zh"]
                else:
                    message = Prompts.return_shades_update_prompt(
                        system_prompt=self.langfuse_dict["shades_update"]["system_prompt"],
                        cur_shades=input_shades,
                        topics_list=input_topics,
                        prefer_lang=preferredLanguage
                    )
                    # å¯æœåŠ¡
                    llm = self.llms["shades_update"]

                    # æ­¥éª¤2: è°ƒç”¨LLMç”Ÿæˆå†…å®¹
                pbar.set_description("è°ƒç”¨LLMæ›´æ–°shade")
                try:
                    answer = llm.chat.completions.create(
                        model=self.model_name,
                        messages=message,
                    )
                    result = answer.choices[0].message.content

                    # process llm result
                    try:
                        logger.info(f"llm return: result: {result}")
                        cleaned_result = self._clean_json_response(result)
                        pre_shades = json.loads(cleaned_result)

                    except json.JSONDecodeError as e:
                        logger.error(f"Failed to parse LLM response: {e}")
                        raise e
                except Exception as e:
                    logger.error(f"Cannot fetch LLM correctly: {e}")
                    raise e
                pbar.update(1)

            # æ­¥éª¤3: ä¸ºå½“å‰shadeåŒ¹é…related Memory
            pbar.set_description("åŒ¹é…ç›¸å…³è®°å¿†")
            for shade in pre_shades:
                # è·å–å½“å‰shadeçš„sourceTopics
                source_topics = shade["sourceTopics"]

                # è·å–å½“å‰shadeçš„relatedNotes
                related_notes = []
                for topic in source_topics:
                    for cur_topic in topics:
                        # print(cur_topic)
                        if cur_topic["topicName"] == topic:
                            related_notes.extend(cur_topic["relatedNotes"])
                shade["relatedNotes"] = related_notes
            pbar.update(1)

            # æ­¥éª¤4: å»é‡å¤„ç†
            pbar.set_description("å¤„ç†é‡å¤è®°å¿†")
            final_result = {}
            final_result["shades"] = pre_shades
            # ä¸ºrelated memory  å»é‡
            for shade in pre_shades:
                related_notes = shade["relatedNotes"]
                related_notes = list(set(related_notes))
                shade["relatedNotes"] = related_notes
            pbar.update(1)

            pbar.set_description("shadeç”Ÿæˆå®Œæˆ")

        logger.info(f"final_result: {final_result}")
        return final_result


class ShadeContentGenerate():
    _input_keys: List[str] = ["noteMemory", "topics", "shades", "preferredLanguage"]
    _output_keys: List[str] = ["shades"]
    _must_keys: List[str] = ["noteMemory", "topics", "shades", "preferredLanguage"]

    Shade_Content_Generate_DEFAULT_PROMPT = {
        "shades_content_generate": Prompts.Shades_Content_SYSTEM_PROMPT,
        "shades_content_generate_zh": Prompts.Shades_Content_zh_SYSTEM_PROMPT,
        "shades_content_update": Prompts.Shades_Content_Update_SYSTEM_PROMPT,
        "shades_content_update_zh": Prompts.Shades_Content_Update_zh_SYSTEM_PROMPT,
    }

    model_params = {
        "max_tokens": 30000,
        "top_p": 0.9,
        "temperature": 0.9,
        "request_timeout": 100,
        "response_format": {"type": "json_object"},
        "max_retries": 1
    }

    @property
    def _api_type(self):
        return "shades_content_generate"

    @property
    def input_keys(self) -> List[str]:
        return self._input_keys

    @property
    def output_keys(self) -> List[str]:
        return self._output_keys

    @property
    def must_keys(self) -> List[str]:
        return self._must_keys

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.max_threads = 8
        self.user_llm_config_service = UserLLMConfigService()
        self.user_llm_config = self.user_llm_config_service.get_available_llm()

        self.llms: Dict[str, openai.OpenAI] = {
            k: openai.OpenAI(
                api_key=self.user_llm_config.chat_api_key,
                base_url=self.user_llm_config.chat_endpoint,
            )
            for k, _ in self.Shade_Content_Generate_DEFAULT_PROMPT.items()
        }
        self.model_name = self.user_llm_config.chat_model_name

        self.class_name = self.__class__.__name__
        self._tokenizer = tiktoken.get_encoding("cl100k_base")
        self.model_params.update(**kwargs)
        self.token_threshold = kwargs.get("token_threshold", 10000)  # é»˜è®¤tokené˜ˆå€¼

        # åˆå§‹åŒ–langfuseå­—å…¸
        self.langfuse_dict = {
            k: self._get_langfuse_prompt(k, v) for k, v in self.Shade_Content_Generate_DEFAULT_PROMPT.items()
        }

    def _get_langfuse_prompt(self, prompt_key, default_prompt) -> Dict[str, Any]:
        # ç›´æ¥ä½¿ç”¨é»˜è®¤æç¤ºè¯ï¼Œä¸å†å°è¯•ä» langfuse è·å–
        system_prompt = default_prompt
        logger.info(f"Using local default prompt for {prompt_key}")
        return {
            "lf_prompt": None,
            "system_prompt": system_prompt
        }

    def _clean_json_response(self, response: str) -> str:
        """
        æ¸…ç†LLMè¿”å›ç»“æœä¸­çš„markdownä»£ç å—æ ‡è®°

        Args:
            response: LLMè¿”å›çš„åŸå§‹å­—ç¬¦ä¸²

        Returns:
            str: æ¸…ç†åçš„JSONå­—ç¬¦ä¸²
        """
        # ç§»é™¤å¼€å¤´çš„```jsonæˆ–```
        if response.strip().startswith('```json'):
            response = response.strip()[7:]  # ç§»é™¤```json
        elif response.strip().startswith('```'):
            response = response.strip()[3:]  # ç§»é™¤```

        # ç§»é™¤ç»“å°¾çš„```
        if response.strip().endswith('```'):
            response = response.strip()[:-3]  # ç§»é™¤ç»“å°¾çš„```

        # Fix unquoted shadeIcon values
        import re
        # Pattern to match "shadeIcon": followed by an unquoted emoji or text
        # This handles cases like "shadeIcon": ğŸ™ï¸, or "shadeIcon": â¤ï¸
        pattern = r'"shadeIcon":\s*([^",\s][^,}\]]*?)(?=,|\s*[}\]])'
        
        def replace_icon(match):
            icon_value = match.group(1).strip()
            # If the icon is not already quoted, quote it
            if not (icon_value.startswith('"') and icon_value.endswith('"')):
                return f'"shadeIcon": "{icon_value}"'
            return match.group(0)
        
        response = re.sub(pattern, replace_icon, response)

        # Additional JSON cleaning - fix common issues
        # Fix trailing commas before closing brackets/braces
        response = re.sub(r',(\s*[}\]])', r'\1', response)
        
        # Note: Removed the general unquoted string fix as it was causing issues with array elements
        # The shadeIcon-specific fix should be sufficient for this use case

        return response.strip()

    def _precess_batch(self, memories: List[Document], topics: List[dict], shades: List[dict]):
        """
        å¤„ç†memoryæ‰¹æ¬¡ï¼Œæ ¹æ®tokenæ•°é‡å†³å®šæ˜¯å¦éœ€è¦åˆ†æ‰¹å¤„ç†
        Args:
            memories: è®°å¿†åˆ—è¡¨
            topics: ä¸»é¢˜åˆ—è¡¨
            shades: æ ‡ç­¾åˆ—è¡¨
        """
        # é¦–å…ˆå¤„ç†æ‰€æœ‰memory
        processed_memories, processed_topics, processed_shades = self._process_memory_format(memories, topics, shades)
        memory_batches = []

        # è®¡ç®—å¤„ç†åçš„memoryçš„tokenæ•°é‡
        memory_tokens = len(self._tokenizer.encode(processed_memories))
        logger.info(f"Current shade related memories' token: {memory_tokens}")

        # å¦‚æœtokenæ•°é‡å°äºé˜ˆå€¼ï¼Œç›´æ¥è¿”å›å¤„ç†åçš„ç»“æœ
        if memory_tokens <= self.token_threshold:
            logger.info("Memory tokens within threshold, process all memories")
            memory_batches.append(processed_memories)
            return memory_batches, processed_topics, processed_shades

        # å¦‚æœtokenæ•°é‡è¶…è¿‡é˜ˆå€¼ï¼Œéœ€è¦åˆ†æ‰¹å¤„ç†
        logger.info(f"Memory tokens exceed threshold ({self.token_threshold}), processing in batches")

        # å°†æ¯æ¡è®°å¿†å•ç‹¬å¤„ç†å¹¶è®¡ç®—token
        memory_str = ""
        memory_str_list = []
        memory_token_list = []

        # å¤„ç†æ¯æ¡è®°å¿†å¹¶è®¡ç®—token
        for memory in memories:
            if "noteId" in memory:  # å¤„ç†ç¬”è®°è®°å¿†
                memory_str = f"Note {memory.get('noteId', '')} Summary: {memory.get('summary', '')}\n"

            memory_tokens = len(self._tokenizer.encode(memory_str))
            memory_str_list.append(memory_str)
            memory_token_list.append(memory_tokens)

        # æŒ‰tokené™åˆ¶åˆ†ç»„
        current_batch = []
        current_batch_tokens = 0

        for memory_str, memory_tokens in zip(memory_str_list, memory_token_list):
            # å¦‚æœå½“å‰è®°å¿†çš„tokenæ•°å·²ç»è¶…è¿‡é˜ˆå€¼ï¼Œéœ€è¦å•ç‹¬ä½œä¸ºä¸€ä¸ªbatch
            if memory_tokens > self.token_threshold:
                if current_batch:  # å¦‚æœå½“å‰batchä¸ä¸ºç©ºï¼Œå…ˆä¿å­˜
                    memory_batches.append("".join(current_batch))
                    current_batch = []
                    current_batch_tokens = 0
                # å°†è¶…é•¿çš„è®°å¿†å•ç‹¬ä½œä¸ºä¸€ä¸ªbatch
                memory_batches.append(memory_str)
                continue

            # å¦‚æœåŠ å…¥å½“å‰è®°å¿†ä¼šè¶…è¿‡é˜ˆå€¼ï¼Œä¿å­˜å½“å‰batchå¹¶å¼€å§‹æ–°çš„batch
            if current_batch_tokens + memory_tokens > self.token_threshold:
                memory_batches.append("".join(current_batch))
                current_batch = [memory_str]
                current_batch_tokens = memory_tokens
            else:
                current_batch.append(memory_str)
                current_batch_tokens += memory_tokens

        # ä¿å­˜æœ€åä¸€ä¸ªbatch
        if current_batch:
            memory_batches.append("".join(current_batch))

        logger.info(f"Split into {len(memory_batches)} batches")
        for i, batch in enumerate(memory_batches):
            batch_tokens = len(self._tokenizer.encode(batch))
            logger.info(f"Batch {i + 1} tokens: {batch_tokens}")

        return memory_batches, processed_topics, processed_shades

    def _process_memory_format(self, memories: List[Document], topics: List[dict], shades: List[dict]) -> tuple[
        str | Any, str | Any, list[dict]]:
        processed_memories = ""
        for memory in memories:
            processed_memories += f"Note {memory.id} Summary: {memory.raw_content}\n"
        processed_topics = ""
        for topic in topics:
            processed_topics += f"Topic {topic.get('topicName', '')} Description: {topic.get('topicDescription', '')}\n"

        return processed_memories, processed_topics, shades

    def _call_llm(self, llm, message, shades: list[dict]):

        # è°ƒç”¨LLMç”Ÿæˆå†…å®¹
        try:
            answer = llm.chat.completions.create(
                model=self.model_name,
                messages=message,
            )
            result = answer.choices[0].message.content

            # process llm result
            try:
                logger.info(f"llm return: result: {result}")
                cleaned_result = self._clean_json_response(result)
                result = json.loads(cleaned_result)
                result = result[0]  # ç†è®ºä¸Šåªæœ‰ä¸€ä¸ªshade

            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse LLM response: {e}")
                logger.info("LLM return parse error, use description as content")
                # è¿”å›é»˜è®¤ç»“æ„ï¼Œé¿å…resultæœªå®šä¹‰
                result = {
                    "shadeContent": "",
                    "shadeContentThirdView": ""
                }

        except Exception as e:
            logger.error(f"Cannot fetch LLM correctly: {e}")
            logger.info("Fetch LLM failed, use description as content")
            # è¿”å›é»˜è®¤ç»“æ„ï¼Œé¿å…resultæœªå®šä¹‰
            result = {
                "shadeContent": "",
                "shadeContentThirdView": ""
            }

        return result

    # @observe(name="shades_content_generate")
    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        # å…¥å‚è·å–
        noteMemory = inputs.get("noteMemory", [])
        topics = inputs.get("topics", [])
        shades = inputs.get("shades", [])
        preferredLanguage = inputs.get("preferredLanguage", "English")

        # ä½¿ç”¨æ‰¹å¤„ç†åŠŸèƒ½å¤„ç†memory
        memory_batches, processed_topics, processed_shades = self._precess_batch(noteMemory, topics, shades)

        logger.info("Multiple batches, process each batch")
        cur_shades = processed_shades
        total_batches = len(memory_batches)

        for batch_idx, memory_batch in enumerate(memory_batches, 1):
            logger.info(f"------ Processing batch {batch_idx}/{total_batches} -------")
            # ç¬¬ä¸€ä¸ªbatchä½¿ç”¨generateï¼Œåç»­batchä½¿ç”¨update
            if batch_idx == 1:
                logger.info("First batch, using generate prompt")
                if preferredLanguage == "ç®€ä½“ä¸­æ–‡/Simplified Chinese" or "chinese":
                    message = Prompts.return_shades_content_generate_prompt(
                        system_prompt=self.langfuse_dict["shades_content_generate_zh"]["system_prompt"],
                        topics_list=processed_topics,
                        cur_shade=cur_shades,
                        related_memories=memory_batch,
                        prefer_lang=preferredLanguage
                    )
                    llm = self.llms["shades_content_generate_zh"]
                else:
                    message = Prompts.return_shades_content_generate_prompt(
                        system_prompt=self.langfuse_dict["shades_content_generate"]["system_prompt"],
                        topics_list=processed_topics,
                        cur_shade=cur_shades,
                        related_memories=memory_batch,
                        prefer_lang=preferredLanguage
                    )
                    llm = self.llms["shades_content_generate"]
            else:
                logger.info(f"Subsequent batch {batch_idx}, using update prompt")
                if preferredLanguage == "ç®€ä½“ä¸­æ–‡/Simplified Chinese" or "chinese":
                    message = Prompts.return_shades_content_update_prompt(
                        system_prompt=self.langfuse_dict["shades_content_update_zh"]["system_prompt"],
                        cur_shade=cur_shades,
                        related_memories=memory_batch,
                        prefer_lang=preferredLanguage
                    )
                    llm = self.llms["shades_content_update_zh"]
                else:
                    message = Prompts.return_shades_content_update_prompt(
                        system_prompt=self.langfuse_dict["shades_content_update"]["system_prompt"],
                        cur_shade=cur_shades,
                        related_memories=memory_batch,
                        prefer_lang=preferredLanguage
                    )
                    llm = self.llms["shades_content_update"]

            cur_shades = self._call_llm(llm, message, processed_shades)
            processed_shades[0]["shadeContent"] = cur_shades.get("shadeContent", "")
            processed_shades[0]["shadeContentThirdView"] = cur_shades.get("shadeContentThirdView", "")

        logger.info(f"------ All batch Process completed -------")
        shades[0]["shadeContent"] = cur_shades.get("shadeContent", "")
        shades[0]["shadeContentThirdView"] = cur_shades.get("shadeContentThirdView", "")

        final_result = {}
        final_result["shades"] = shades
        return final_result


if __name__ == "__main__":

    doc_repository = DocumentRepository()
    documents = doc_repository.list()

    topics_path = "resources/data/stage2/topics/topic.json"
    with open(topics_path, "r") as f:
        topics_data = json.load(f)
        topics = topics_data.get("topics", [])

    # æµ‹è¯•shade generate
    shade_generate = ShadeGenerate()
    shades = []
    preferredLanguage = "ç®€ä½“ä¸­æ–‡/Simplified Chinese"
    shades_result = shade_generate.shades_generate(topics=topics, shades=shades, preferredLanguage=preferredLanguage)
    logger.info(f"shades generate result: {shades_result}")
    # ä¿å­˜shades result
    shades_result_json = json.dumps(shades_result, ensure_ascii=False, indent=2)
    os.makedirs("resources/data/stage2/shades", exist_ok=True)
    with open("resources/data/stage2/shades/shades.json", "w", encoding="utf-8") as f:
        f.write(shades_result_json)

    with open("resources/data/stage2/shades/shades.json", "r", encoding="utf-8") as f:
        shades_result = json.load(f)
    final_result = shades_result
    shade_content_generate = ShadeContentGenerate()
    for idx, shade in enumerate(shades_result["shades"]):

        cur_note = []
        cur_topic = []

        for note in documents:
            if note.id in shade["relatedNotes"]:
                cur_note.append(note)
        for topic in topics:
            if topic["topicName"] in shade["sourceTopics"]:
                cur_topic.append(topic)
        shades_content_input = {
            "noteMemory": cur_note,
            "topics": cur_topic,
            "shades": [shade],
            "preferredLanguage": "ç®€ä½“ä¸­æ–‡/Simplified Chinese"
        }
        shades_content_result = shade_content_generate._call(shades_content_input)
        logger.info(f"shade content generate result: {shades_content_result}")
        final_result["shades"][idx]["shadeContent"] = shades_content_result["shades"][0]["shadeContent"]
        final_result["shades"][idx]["shadeContentThirdView"] = shades_content_result["shades"][0][
            "shadeContentThirdView"]
    # ä¿å­˜shades content result
    with open("resources/data/stage2/shades/shades_content.json", "w", encoding="utf-8") as f:
        json.dump(final_result, f, indent=4, ensure_ascii=False)
