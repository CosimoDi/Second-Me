#项目复现逻辑

1. 下载并解压，clone 失败了就手动下载
2. 安装docker，并配置好wsl
3. 在项目文件夹的终端下，运行 make docker-up
4. 访问 localhost:3000 

docker在切换IP（即WiFi）时，会导致登录失败。
需要到：
C:\Users\Cosimo\AppData\Local\Docker\log
这个地址去找报错，然后和copilot对话，它会告诉你，你在settings-store.json中设置了代理：
也有可能是json坏了，反正在这个地址去找就ok，实在不行只能重装了，鬼知道什么出问题了。


例：
{
  "OverrideProxyHTTP": "http://192.168.2.101:16780",
  "OverrideProxyHTTPS": "http://192.168.2.101:16780",
  "ProxyHTTPMode": "manual"
}
让co-pilot帮忙把代理改了就好。一般是直接删掉旧的代理地址，然后再登录。


API优先选硅基流动，之前尝试用poloai，想用它的gpt5.2失败，然后用了ollama，卡了一个星期在最后一步。直接用硅基流动的话至少省下我四五天，国内AI就国内AI吧，至少API是稳定的。比折腾本地也好很多。
坏，解析json失败了，当我没说。




#ollama血泪史：
Custom model配置
Chat	qwen3:8b	http://192.168.2.101:11434/v1
Embedding	qwen3-embedding:latest	http://192.168.2.101:11434/v1
Thinking	http://192.168.2.101:11434/v1
#训练过程：
注：openai的模型因搞不到api无法复现，使用自定义模型进行训练
    在整个运行过程中，最好不要改变IP地址。（包括更换梯子）

1. Custom模型需要下载ollama

2. 先运行ollama ，代码如下：

# 1. 终止残留进程

taskkill /F /IM ollama.exe 2>&1 | Out-Null
Start-Sleep -Seconds 3

# 2. 清空旧环境变量（先检查是否存在，避免删除报错）
if (Test-Path Env:OLLAMA_CUDA) { Remove-Item Env:OLLAMA_CUDA }
if (Test-Path Env:OLLAMA_MAX_GPU) { Remove-Item Env:OLLAMA_MAX_GPU }

# 3. 适配RTX 4060 8G显存的GPU配置
$env:OLLAMA_OPENAI_COMPATIBLE=1
$env:OLLAMA_GPU=90
$env:OLLAMA_OFFLOAD_ALL=1
$env:OLLAMA_HOST="0.0.0.0:11434"
$env:OLLAMA_NUM_GPU=1

# 4. 启动Ollama并在控制台实时输出日志（按要求调整颜色）
Write-Host "🔄 正在启动Ollama服务...（控制台实时输出日志）" -ForegroundColor Cyan
& "$env:LOCALAPPDATA\Programs\Ollama\ollama.exe" serve 2>&1 | ForEach-Object {
    if ($_ -notmatch "failed to get console mode for stderr") {
        if ($_ -match "level=INFO") {
            Write-Host $_ -ForegroundColor White
        } elseif ($_ -match "level=WARN") {
            Write-Host $_ -ForegroundColor Yellow
        } elseif ($_ -match "level=ERROR") {
            Write-Host $_ -ForegroundColor Red
        } else {
            Write-Host $_ -ForegroundColor Blue
        }
    }
}


3.根据自己的IP地址，修改endpoint。并另起一个powershell，输入例：
 Invoke-RestMethod -Uri "http://192.168.2.101:11434/v1/models" -Method Get

模型启动

4. 在训练窗口下配置好 api=ollama，endpoint=http://你的ip:11434，model=custom-model-name

##训练过程可能会卡住的地方：
登录的时候报错500，原因是之前训练的时候93%卡住了，然后直接退出的。
建议：
git log 

# 回退到 n 个提交之前
git reset --hard HEAD~n

# 或者回退到具体的提交
git reset --hard abc1234


Generate Document Embeddings
1. 有可能是前面配置endpoint的时候，没有 $env:OLLAMA_HOST = "0.0.0.0:11434"导致docker上的项目无法访问ollama
2. IP地址错误，这个是我目前的地址192.168.2.101:11434
3. ollama卡住了，需要在启动ollama的powershell下，按enter
（配置好了之后，这一步其实很快，一个文件大概半秒就完了，然后是下一个可能会卡住的地方）

Generate Biography
这里通常是因为下载的对话模型你电脑带不动，太慢了导致报错500
解决办法：
1. 换一个轻量级的模型
2. 用命令把CPU换成全GPU跑
3. 在vscode里让co-pilot帮改代码，调整一下超时时间

Map Your Entity Network
这里是单纯的比较慢，可以等一会儿，风扇在转，powershell有更新说明没问题。
5分钟搞定

Prepare Training Data for Deep Comprehension
这一步略久，本身是因为数据量不够需要额外生成一些虚拟的数据给后续训练使用，五分钟左右生成足够的数据

Train
我讨厌你，93%·····,这一步实际上卡了我好几天，第一次是发现自己的CUDA适配。
第二次是下面这个报错：
2026-01-26 01:12:03 [INFO] trainprocess_service.py:1179 - Training process has been requested to stop
2026-01-26 01:12:03 [INFO] trainprocess_service.py:1196 - Attempting to terminate process with PID: 815
发现可能是代码里写的关于PID的识别出错了，把应该识别子进程的PID写成了主进程的PID，导致无法正确终止进程。